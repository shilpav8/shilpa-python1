{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33e6d01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU langchain langchain-community langchain-chroma\n",
    "!pip install -qU sentence-transformers\n",
    "!pip install -qU tiktoken\n",
    "\n",
    "# For free LLM model\n",
    "!pip install -qU transformers accelerate sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b2b6367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 1\n",
      "Sample content:\n",
      " Build a RAG agent with LangChain - Docs by LangChainSkip to main contentðŸš€ Share how you're building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentSupervisor agentLangGraphConceptual overviewsComponent architectureMemoryC\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "url = \"https://python.langchain.com/docs/tutorials/rag/\"\n",
    "loader = WebBaseLoader(url)\n",
    "\n",
    "raw_docs = loader.load()\n",
    "\n",
    "print(\"Number of documents:\", len(raw_docs))\n",
    "print(\"Sample content:\\n\", raw_docs[0].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d636f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning:\n",
      "\n",
      "Build a RAG agent with LangChain - Docs by LangChainSkip to main contentðŸš€ Share how you're building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainL\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    # remove line breaks\n",
    "    text = text.replace(\"\\n\", \" \").strip()\n",
    "    # collapse multiple spaces into one\n",
    "    while \"  \" in text:\n",
    "        text = text.replace(\"  \", \" \")\n",
    "    return text\n",
    "\n",
    "# apply cleaning to every document\n",
    "for doc in raw_docs:\n",
    "    doc.page_content = clean_text(doc.page_content)\n",
    "\n",
    "print(\"After cleaning:\\n\")\n",
    "print(raw_docs[0].page_content[:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd8d6d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 36\n",
      "\n",
      "Sample chunk:\n",
      " Build a RAG agent with LangChain - Docs by LangChainSkip to main contentðŸš€ Share how you're building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemanti\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,      \n",
    "    chunk_overlap=200,   \n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(raw_docs)\n",
    "\n",
    "print(\"Total chunks:\", len(chunks))\n",
    "print(\"\\nSample chunk:\\n\", chunks[0].page_content[:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da2bc189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length: 384\n",
      "First 5 numbers: [-0.11101916432380676, -0.026313427835702896, -0.05789431557059288, 0.05978523567318916, -0.020831575617194176]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# free embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Quick test\n",
    "test_vector = embedding_model.embed_query(\"What is Retrieval-Augmented Generation?\")\n",
    "print(\"Embedding length:\", len(test_vector))\n",
    "print(\"First 5 numbers:\", test_vector[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59beed93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vector store created!\n",
      "\n",
      "Retrieved 3 chunks.\n",
      "\n",
      "Sample retrieved chunk:\n",
      "\n",
      "Build a RAG agent with LangChain - Docs by LangChainSkip to main contentðŸš€ Share how you're building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentSupervisor agentLangGraphConceptual overviewsComponent architectureMemoryC\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "# create vector store from your chunks\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model,\n",
    "    collection_name=\"langchain_rag_docs\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Vector store created!\")\n",
    "\n",
    "# --- test retrieval ---\n",
    "query = \"What are the main steps to build a RAG pipeline?\"\n",
    "docs_retrieved = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(\"\\nRetrieved\", len(docs_retrieved), \"chunks.\")\n",
    "print(\"\\nSample retrieved chunk:\\n\")\n",
    "print(docs_retrieved[0].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "733fbe22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever returned 3 docs.\n",
      "\n",
      "First doc sample:\n",
      "\n",
      "'9000bf8e-1993-446f-8d4d-f4e507ba4b8f', 'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6'] Go deeper Embeddings: Wrapper around a text embedding model, used for converting text to embeddings. Integrations: 30+ integrations to choose from. Interface: API reference for the base interface. VectorStore: Wrapper around a vector database, used for storing and querying embeddings. Integrations: 40+ integrations to choose from. Interface: API reference for the base interface. This completes the Indexing portion of\n"
     ]
    }
   ],
   "source": [
    "# turn vector store into a retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",      # basic similarity search\n",
    "    search_kwargs={\"k\": 3}         # return top 3 chunks\n",
    ")\n",
    "\n",
    "# test retriever\n",
    "test_query = \"How does chunking work in a RAG pipeline?\"\n",
    "retrieved_docs = retriever.invoke(test_query)\n",
    "\n",
    "print(\"Retriever returned\", len(retrieved_docs), \"docs.\\n\")\n",
    "print(\"First doc sample:\\n\")\n",
    "print(retrieved_docs[0].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93a4c164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading model (this one is heavier than previous steps)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TinyLlama loaded and ready!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"Loading model (this one is heavier than previous steps)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\"   # uses GPU if available, else CPU\n",
    ")\n",
    "\n",
    "llm_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256\n",
    ")\n",
    "\n",
    "print(\"âœ… TinyLlama loaded and ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65a986cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RAG function defined.\n"
     ]
    }
   ],
   "source": [
    "def format_docs(docs):\n",
    "    # Join all retrieved chunks with separators\n",
    "    return \"\\n\\n---\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "system_instruction = (\n",
    "    \"You are an assistant that answers questions about LangChain's RAG tutorial. \"\n",
    "    \"Use ONLY the context below. If you are not sure from the context, say you are not sure.\"\n",
    ")\n",
    "\n",
    "def rag_answer(question: str) -> str:\n",
    "    # 1. Retrieve relevant chunks\n",
    "    docs = retriever.invoke(question)\n",
    "    context = format_docs(docs)\n",
    "\n",
    "    # 2. Build prompt for TinyLlama\n",
    "    prompt = (\n",
    "        f\"{system_instruction}\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "\n",
    "    # 3. Call LLM\n",
    "    outputs = llm_pipeline(prompt)[0][\"generated_text\"]\n",
    "\n",
    "    # 4. Simple cleanup: return text after \"Answer:\"\n",
    "    if \"Answer:\" in outputs:\n",
    "        return outputs.split(\"Answer:\", 1)[-1].strip()\n",
    "    else:\n",
    "        return outputs.strip()\n",
    "\n",
    "print(\"âœ… RAG function defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f689816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n---\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "system_instruction = (\n",
    "    \"You are an assistant that answers questions about LangChain's RAG tutorial. \"\n",
    "    \"Use ONLY the context below. \"\n",
    "    \"Give a short, clear answer in 3â€“6 sentences. \"\n",
    "    \"Do not ask new questions. Do not start new Q&A pairs.\"\n",
    ")\n",
    "\n",
    "def rag_answer(question: str) -> str:\n",
    "    # 1. Retrieve relevant chunks\n",
    "    docs = retriever.invoke(question)\n",
    "    context = format_docs(docs)\n",
    "\n",
    "    # 2. Build prompt\n",
    "    prompt = (\n",
    "        f\"{system_instruction}\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "\n",
    "    # 3. Call TinyLlama\n",
    "    outputs = llm_pipeline(prompt)[0][\"generated_text\"]\n",
    "\n",
    "    # 4. Take only the text after \"Answer:\"\n",
    "    text = outputs.split(\"Answer:\", 1)[-1].strip()\n",
    "\n",
    "    # 5. If it starts adding another \"Question:\", cut it off\n",
    "    if \"Question:\" in text:\n",
    "        text = text.split(\"Question:\", 1)[0].strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35477e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: langchain?\n",
      "A: Add a key to the state to store the retrieved documents and add a new node via a pre-model hook to populate that key (as well as inject the context).\n"
     ]
    }
   ],
   "source": [
    "#print(\"Q: What is Retrieval-Augmented Generation (RAG)?\")\n",
    "#print(\"A:\", rag_answer(\"What is Retrieval-Augmented Generation (RAG)?\"))\n",
    "print(\"Q: langchain?\")\n",
    "print(\"A:\", rag_answer(\"langchain\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
