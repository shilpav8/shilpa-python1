{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33e6d01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU langchain langchain-community langchain-chroma\n",
    "!pip install -qU sentence-transformers\n",
    "!pip install -qU tiktoken\n",
    "\n",
    "# For free LLM model\n",
    "!pip install -qU transformers accelerate sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b2b6367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 1\n",
      "Sample content:\n",
      " Build a RAG agent with LangChain - Docs by LangChainSkip to main contentðŸš€ Share how you're building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentSupervisor agentLangGraphConceptual overviewsComponent architectureMemoryC\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "url = \"https://python.langchain.com/docs/tutorials/rag/\"\n",
    "loader = WebBaseLoader(url)\n",
    "\n",
    "raw_docs = loader.load()\n",
    "\n",
    "print(\"Number of documents:\", len(raw_docs))\n",
    "print(\"Sample content:\\n\", raw_docs[0].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d636f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning:\n",
      "\n",
      "Build a RAG agent with LangChain - Docs by LangChainSkip to main contentðŸš€ Share how you're building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainL\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    # remove line breaks\n",
    "    text = text.replace(\"\\n\", \" \").strip()\n",
    "    # collapse multiple spaces into one\n",
    "    while \"  \" in text:\n",
    "        text = text.replace(\"  \", \" \")\n",
    "    return text\n",
    "\n",
    "# apply cleaning to every document\n",
    "for doc in raw_docs:\n",
    "    doc.page_content = clean_text(doc.page_content)\n",
    "\n",
    "print(\"After cleaning:\\n\")\n",
    "print(raw_docs[0].page_content[:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd8d6d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 36\n",
      "\n",
      "Sample chunk:\n",
      " Build a RAG agent with LangChain - Docs by LangChainSkip to main contentðŸš€ Share how you're building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemanti\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,      \n",
    "    chunk_overlap=200,   \n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(raw_docs)\n",
    "\n",
    "print(\"Total chunks:\", len(chunks))\n",
    "print(\"\\nSample chunk:\\n\", chunks[0].page_content[:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da2bc189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shilc\\AppData\\Local\\Temp\\ipykernel_20332\\3084131981.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length: 384\n",
      "First 5 numbers: [-0.11101916432380676, -0.026313427835702896, -0.05789431557059288, 0.05978523567318916, -0.020831575617194176]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# free embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Quick test\n",
    "test_vector = embedding_model.embed_query(\"What is Retrieval-Augmented Generation?\")\n",
    "print(\"Embedding length:\", len(test_vector))\n",
    "print(\"First 5 numbers:\", test_vector[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59beed93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vector store created!\n",
      "\n",
      "Retrieved 3 chunks.\n",
      "\n",
      "Sample retrieved chunk:\n",
      "\n",
      "Build a RAG agent with LangChain - Docs by LangChainSkip to main contentðŸš€ Share how you're building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentSupervisor agentLangGraphConceptual overviewsComponent architectureMemoryC\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "# create vector store from your chunks\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model,\n",
    "    collection_name=\"langchain_rag_docs\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Vector store created!\")\n",
    "\n",
    "# --- test retrieval ---\n",
    "query = \"What are the main steps to build a RAG pipeline?\"\n",
    "docs_retrieved = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(\"\\nRetrieved\", len(docs_retrieved), \"chunks.\")\n",
    "print(\"\\nSample retrieved chunk:\\n\")\n",
    "print(docs_retrieved[0].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "733fbe22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever returned 3 docs.\n",
      "\n",
      "First doc sample:\n",
      "\n",
      "'9000bf8e-1993-446f-8d4d-f4e507ba4b8f', 'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6'] Go deeper Embeddings: Wrapper around a text embedding model, used for converting text to embeddings. Integrations: 30+ integrations to choose from. Interface: API reference for the base interface. VectorStore: Wrapper around a vector database, used for storing and querying embeddings. Integrations: 40+ integrations to choose from. Interface: API reference for the base interface. This completes the Indexing portion of\n"
     ]
    }
   ],
   "source": [
    "# turn vector store into a retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",      # basic similarity search\n",
    "    search_kwargs={\"k\": 3}         # return top 3 chunks\n",
    ")\n",
    "\n",
    "# test retriever\n",
    "test_query = \"How does chunking work in a RAG pipeline?\"\n",
    "retrieved_docs = retriever.invoke(test_query)\n",
    "\n",
    "print(\"Retriever returned\", len(retrieved_docs), \"docs.\\n\")\n",
    "print(\"First doc sample:\\n\")\n",
    "print(retrieved_docs[0].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93a4c164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0bdef190684015a167d88da50ab17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shilc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shilc\\.cache\\huggingface\\hub\\models--TinyLlama--TinyLlama-1.1B-Chat-v1.0. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409449156b6940c9bedef0d92c799cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b742621f40b490980eb28647b0277a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84339c56c9dc4ccfb1187f715e950469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model (this one is heavier than previous steps)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c21ad86a6c473abcd7cbb0a38c0da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63851c66a7204755b2dd4141bcc7ffcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4936e806321f45cba458508724d4ef1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TinyLlama loaded and ready!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"Loading model (this one is heavier than previous steps)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\"   # uses GPU if available, else CPU\n",
    ")\n",
    "\n",
    "llm_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256\n",
    ")\n",
    "\n",
    "print(\"âœ… TinyLlama loaded and ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65a986cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RAG function defined.\n"
     ]
    }
   ],
   "source": [
    "def format_docs(docs):\n",
    "    # Join all retrieved chunks with separators\n",
    "    return \"\\n\\n---\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "system_instruction = (\n",
    "    \"You are an assistant that answers questions about LangChain's RAG tutorial. \"\n",
    "    \"Use ONLY the context below. If you are not sure from the context, say you are not sure.\"\n",
    ")\n",
    "\n",
    "def rag_answer(question: str) -> str:\n",
    "    # 1. Retrieve relevant chunks\n",
    "    docs = retriever.invoke(question)\n",
    "    context = format_docs(docs)\n",
    "\n",
    "    # 2. Build prompt for TinyLlama\n",
    "    prompt = (\n",
    "        f\"{system_instruction}\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "\n",
    "    # 3. Call LLM\n",
    "    outputs = llm_pipeline(prompt)[0][\"generated_text\"]\n",
    "\n",
    "    # 4. Simple cleanup: return text after \"Answer:\"\n",
    "    if \"Answer:\" in outputs:\n",
    "        return outputs.split(\"Answer:\", 1)[-1].strip()\n",
    "    else:\n",
    "        return outputs.strip()\n",
    "\n",
    "print(\"âœ… RAG function defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f689816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n---\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "system_instruction = (\n",
    "    \"You are an assistant that answers questions about LangChain's RAG tutorial. \"\n",
    "    \"Use ONLY the context below. \"\n",
    "    \"Give a short, clear answer in 3â€“6 sentences. \"\n",
    "    \"Do not ask new questions. Do not start new Q&A pairs.\"\n",
    ")\n",
    "\n",
    "def rag_answer(question: str) -> str:\n",
    "    # 1. Retrieve relevant chunks\n",
    "    docs = retriever.invoke(question)\n",
    "    context = format_docs(docs)\n",
    "\n",
    "    # 2. Build prompt\n",
    "    prompt = (\n",
    "        f\"{system_instruction}\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "\n",
    "    # 3. Call TinyLlama\n",
    "    outputs = llm_pipeline(prompt)[0][\"generated_text\"]\n",
    "\n",
    "    # 4. Take only the text after \"Answer:\"\n",
    "    text = outputs.split(\"Answer:\", 1)[-1].strip()\n",
    "\n",
    "    # 5. If it starts adding another \"Question:\", cut it off\n",
    "    if \"Question:\" in text:\n",
    "        text = text.split(\"Question:\", 1)[0].strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35477e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: langchain?\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'rag_answer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#print(\"Q: What is Retrieval-Augmented Generation (RAG)?\")\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#print(\"A:\", rag_answer(\"What is Retrieval-Augmented Generation (RAG)?\"))\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mQ: langchain?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mA:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mrag_answer\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mlangchain\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'rag_answer' is not defined"
     ]
    }
   ],
   "source": [
    "#print(\"Q: What is Retrieval-Augmented Generation (RAG)?\")\n",
    "#print(\"A:\", rag_answer(\"What is Retrieval-Augmented Generation (RAG)?\"))\n",
    "print(\"Q: langchain?\")\n",
    "print(\"A:\", rag_answer(\"langchain\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
